
<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description"
    content="FRAME: Feature Representation and Anticipation with Memory - A self-supervised video frame encoder for dense video understanding">
  <meta property="og:title" content="FRAME: Feature Representation and Anticipation with Memory" />
  <meta property="og:description"
    content="A self-supervised video frame encoder that learns temporally consistent, spatially dense features for dense video prediction tasks" />
  <meta property="og:url" content="" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/FRAME_teaser.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />

  <meta name="twitter:title" content="FRAME: Feature Representation and Anticipation with Memory">
  <meta name="twitter:description" content="A self-supervised video frame encoder for dense video understanding">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/FRAME_teaser.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords"
    content="computer vision, video understanding, self-supervised learning, dense prediction, video segmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>FRAME: Feature Representation and Anticipation with Memory</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  
  <style>
    /* Custom animations and styles */
    @keyframes fadeInUp {
      from {
        opacity: 0;
        transform: translateY(30px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }
    
    @keyframes slideInLeft {
      from {
        opacity: 0;
        transform: translateX(-50px);
      }
      to {
        opacity: 1;
        transform: translateX(0);
      }
    }
    
    @keyframes slideInRight {
      from {
        opacity: 0;
        transform: translateX(50px);
      }
      to {
        opacity: 1;
        transform: translateX(0);
      }
    }
    
    @keyframes scaleIn {
      from {
        opacity: 0;
        transform: scale(0.8);
      }
      to {
        opacity: 1;
        transform: scale(1);
      }
    }
    
    .fade-in-up {
      animation: fadeInUp 0.8s ease-out;
    }
    
    .slide-in-left {
      animation: slideInLeft 0.8s ease-out;
    }
    
    .slide-in-right {
      animation: slideInRight 0.8s ease-out;
    }
    
    .scale-in {
      animation: scaleIn 0.6s ease-out;
    }
    
    .hero-body {
      animation: fadeInUp 1s ease-out;
    }
    
    .publication-title {
      animation: fadeInUp 1.2s ease-out;
    }
    
    .publication-authors {
      animation: fadeInUp 1.4s ease-out;
    }
    
    .publication-links {
      animation: fadeInUp 1.6s ease-out;
    }
    
    /* Hover effects */
    .external-link:hover {
      transform: translateY(-2px);
      transition: transform 0.3s ease;
    }
    
    .table tr:hover {
      transform: scale(1.02);
      transition: transform 0.2s ease;
    }
    
    /* Video hover effects */
    video:hover {
      transform: scale(1.05);
      transition: transform 0.3s ease;
      box-shadow: 0 10px 30px rgba(0,0,0,0.3);
    }
    
    /* Image hover effects */
    img:hover {
      transform: scale(1.02);
      transition: transform 0.3s ease;
    }
    
    /* Floating animation for teaser */
    .teaser-float {
      animation: float 6s ease-in-out infinite;
    }
    
    @keyframes float {
      0%, 100% { transform: translateY(0px); }
      50% { transform: translateY(-10px); }
    }
    
    /* Remove custom gradient backgrounds to keep original colors */
    
    /* Staggered animations for tables */
    .table tbody tr {
      animation: fadeInUp 0.6s ease-out;
    }
    
    .table tbody tr:nth-child(1) { animation-delay: 0.1s; }
    .table tbody tr:nth-child(2) { animation-delay: 0.2s; }
    .table tbody tr:nth-child(3) { animation-delay: 0.3s; }
    .table tbody tr:nth-child(4) { animation-delay: 0.4s; }
    .table tbody tr:nth-child(5) { animation-delay: 0.5s; }
    .table tbody tr:nth-child(6) { animation-delay: 0.6s; }
    
    /* Pulse animation for highlighted rows */
    .has-background-success-light {
      animation: pulse 2s ease-in-out infinite alternate;
    }
    
    @keyframes pulse {
      0% { background-color: #d4edda; }
      100% { background-color: #c3e6cb; }
    }

    /* --- Interactive Demo styles --- */
    #dinov3-demo {
      margin-top: 2.5rem;
    }

    #dinov3-status {
      min-height: 1.6rem;
      text-align: center;
      font-weight: 600;
      color: #1a1a1a;
    }

    #dinov3-wrapper {
      margin-top: 1.5rem;
      display: flex;
      flex-direction: column;
      gap: 1rem;
      align-items: stretch;
    }

    #dinov3-video-container {
      position: relative;
      width: 100%;
      aspect-ratio: 16 / 9;
      background: #000;
      border-radius: 12px;
      overflow: hidden;
      cursor: crosshair;
    }

    #dinov3-video,
    #dinov3-overlay {
      position: absolute;
      inset: 0;
      width: 100%;
      height: 100%;
    }

    #dinov3-overlay {
      pointer-events: auto;
    }

    #dinov3-controls {
      display: flex;
      flex-wrap: wrap;
      gap: 1rem;
      align-items: center;
      justify-content: space-between;
    }

    #dinov3-controls button {
      min-width: 90px;
    }

    #dinov3-slider {
      flex: 1;
    }

    #dinov3-picker {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(140px, 1fr));
      gap: 0.75rem;
      margin-top: 0.5rem;
    }

    .dinov3-tile {
      position: relative;
      display: flex;
      flex-direction: column;
      gap: 0.5rem;
      padding: 0.5rem;
      border: none;
      border-radius: 12px;
      background: #f4f4f5;
      cursor: pointer;
      transition: transform 0.2s ease, box-shadow 0.2s ease, outline 0.2s ease;
      outline: 2px solid transparent;
    }

    .dinov3-tile video {
      width: 100%;
      border-radius: 8px;
      aspect-ratio: 16 / 9;
      object-fit: cover;
      pointer-events: none;
    }

    .dinov3-tile span {
      font-weight: 600;
      text-align: center;
      color: #111827;
    }

    .dinov3-tile:hover:not([disabled]) {
      transform: translateY(-2px);
      box-shadow: 0 8px 16px rgba(15, 23, 42, 0.12);
    }

    .dinov3-tile.is-active {
      outline-color: #2563eb;
      box-shadow: 0 0 0 4px rgba(37, 99, 235, 0.12);
      background: #e0ecff;
    }

    .dinov3-tile[disabled] {
      cursor: wait;
      opacity: 0.65;
    }

    /* --- Downstream tasks styles --- */
    #downstream-tasks-section {
      padding: 2.5rem 0;
    }

    #downstream-tasks-section .hero-body {
      padding: 1rem 0;
    }

    #downstream-tasks-section .container {
      max-width: min(1080px, 92vw);
    }

    #downstream-carousel .downstream-card {
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      gap: 1.25rem;
      min-height: clamp(360px, 55vw, 520px);
      padding: 0 1.25rem;
    }

    #downstream-carousel .downstream-card img {
      width: 100%;
      max-width: 980px;
      height: auto;
      border-radius: 18px;
      object-fit: contain;
      box-shadow: 0 18px 40px rgba(15, 23, 42, 0.18);
      background: #fff;
    }

    #downstream-carousel .downstream-caption {
      font-size: clamp(1.05rem, 1.5vw, 1.35rem);
      margin-top: 0.25rem;
      max-width: 760px;
    }
  </style>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">FRAME: Feature Representation and Anticipation with Memory</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="mailto:st34@illinois.edu" target="_blank">Sethuraman T V</a><sup>1,2</sup>,</span>
              <span class="author-block">
                <a href="https://savya08.github.io/" target="_blank">Savya Khosla</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://trevahok.github.io" target="_blank">Vignesh Srinivasakumar</a><sup>2†</sup>,</span>
              <span class="author-block">
                <a href="https://gabriel-huang.github.io/" target="_blank">Jiahui Huang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://sites.google.com/view/seoungwugoh/" target="_blank">Seoung Wug Oh</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://sjenni.github.io/" target="_blank">Simon Jenni</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://dhoiem.cs.illinois.edu/" target="_blank">Derek Hoiem</a><sup>2*</sup>,</span>
              <span class="author-block">
                <a href="https://joonyoung-cv.github.io/" target="_blank">Joon-Young Lee</a><sup>1*</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <span style="color: #FF0000;">Adobe Research</span><sup>1</sup>,
                <span style="color: #13294B;">University of Illinois Urbana-Champaign</span><sup>2</sup><br>
              </span>
              <span class="eql-cntrb">
                <small><br><sup>*</sup>Equal advising, <sup>†</sup>Now at <span
                    style="color: #76B900;">NVIDIA</span></small>
              </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2506.05543" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <span class="link-block">
                  <a href="23370_FRAME_Pre_Training_Video_Supplementary Material.zip" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="#" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser image -->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="scale-in">
          <img src="figures/image.png" alt="FRAME Teaser" class="center teaser-float">
        </div>
        <h2 class="subtitle has-text-centered fade-in-up">
          FRAME outperforms state-of-the-art self-supervised models (DINO, SiamMAE) on multiple dense video tasks. The
          student (FRAME) surpasses the teacher (DINO) by learning to predict current and future features using memory,
          improving temporal consistency and visual correspondence. (Right) Eg: VOS where FRAME improves segmentation of
          horse and rider. (left) Tasks shown: VOS = Video Object Segmentation, Part Prop. = Part Propagation, Pose
          Prop. = Pose Propagation, Seg = Semantic Segmentation of current & future frame.
        </h2>
      </div>
    </div>
  </section>


  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 fade-in-up">Abstract</h2>
          <div class="content has-text-justified slide-in-left">
            <p>
              <strong>The Task:</strong> Dense video prediction tasks, such as object tracking and semantic segmentation, require video encoders that generate temporally consistent, spatially dense features for every frame.
            </p>
            <p>
              <strong>What's Missing:</strong> Existing approaches fall short: image encoders like DINO or CLIP lack temporal awareness, while video models such as VideoMAE underperform compared to image encoders on dense prediction tasks.
            </p>
            <p>
              <strong>Our Solution:</strong> We address this gap with <strong>FRAME</strong>, a self-supervised video frame encoder tailored for dense video understanding. FRAME learns to predict current and future DINO patch features from past and present RGB frames, leading to spatially precise and temporally coherent representations. To our knowledge, FRAME is the first video encoder to leverage image-based models for dense prediction while outperforming them on tasks requiring fine-grained visual correspondence. As an auxiliary capability, FRAME aligns its class token with CLIP's semantic space, supporting language-driven tasks such as video classification.
            </p>
            <p>
              <strong>Our Performance:</strong> We evaluate FRAME across <em>six dense prediction tasks</em> on <em>seven datasets</em>, where it consistently outperforms image encoders and existing self-supervised video models. Despite its versatility, FRAME maintains a compact architecture suitable for a range of downstream applications.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Method Overview -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 fade-in-up">Method Overview</h2>
          <div class="content has-text-justified slide-in-right">
            <p>
              FRAME is trained in <strong>two stages</strong>. In <strong>Stage 1</strong>, we train a student encoder to match dense patch-level and
              class-level features from frozen image-based teacher models (DINO and CLIP). In <strong>Stage 2</strong>, we equip the
              student with lightweight temporal modules—a memory unit that aggregates past context and an anticipation
              unit that predicts future features.
            </p>
          </div>
          <div class="scale-in">
            <img src="figures/framework.png" alt="FRAME Framework" class="center">
          </div>
          <h2 class="subtitle has-text-centered fade-in-up">
            Overview of FRAME Architecture and Two-Stage Training Process. 
          </h2>
        </div>
      </div>
    </div>
  </section>

  <!-- Video carousel - Now placed before downstream tasks -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3 has-text-centered fade-in-up">Video Examples</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-video1 has-text-centered scale-in">
            <video poster="" id="video1" autoplay controls muted loop class="is-centered">
              <source src="static/videos/demo1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video2 has-text-centered scale-in">
            <video poster="" id="video2" autoplay controls muted loop class="is-centered">
              <source src="static/videos/demo2.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End video carousel -->

  <!-- Interactive Demo -->
  <section class="section hero is-light" id="interactive-demo">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered fade-in-up">Interactive Demo</h2>
      <p class="subtitle has-text-centered slide-in-left">
        Experiment with an in-browser FRAME-DINOv3 tracker on a few examples. The video loads and processes automatically&mdash;click anywhere on the subject you want to follow to visualize the similarity heat map.
      </p>

      <div id="dinov3-demo" class="box">
        <div id="dinov3-status">Preparing demo…</div>

        <div id="dinov3-wrapper">
          <div id="dinov3-video-container">
            <video id="dinov3-video" playsinline muted></video>
            <canvas id="dinov3-overlay"></canvas>
          </div>

          <div id="dinov3-controls">
            <button id="dinov3-play" class="button is-dark" disabled>Play</button>
            <input id="dinov3-slider" type="range" min="0" max="0" value="0" disabled>
            <span id="dinov3-counter" class="tag is-dark">Frame 0 / 0</span>
          </div>

          <div id="dinov3-picker" class="slide-in-right">
            <button class="dinov3-tile is-active" data-video-id="demo-12927739" type="button" aria-pressed="true">
              <video src="https://huggingface.co/spaces/webml-community/DINOv3-video-tracking/resolve/main/examples/12927739_60fps.mp4" muted loop playsinline preload="metadata"></video>
              <span>Demo 1</span>
            </button>
            <button class="dinov3-tile" data-video-id="demo-8624901" type="button" aria-pressed="false">
              <video src="https://huggingface.co/spaces/webml-community/DINOv3-video-tracking/resolve/main/examples/8624901_30fps.mp4" muted loop playsinline preload="metadata"></video>
              <span>Demo 2</span>
            </button>
            <button class="dinov3-tile" data-video-id="demo-4411457" type="button" aria-pressed="false">
              <video src="https://huggingface.co/spaces/webml-community/DINOv3-video-tracking/resolve/main/examples/4411457_25fps.mp4" muted loop playsinline preload="metadata"></video>
              <span>Demo 3</span>
            </button>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Interactive Demo -->

<!-- Downstream Tasks (previously Key Results) - Optimized size -->
<section id="downstream-tasks-section" class="hero is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered fade-in-up" style="margin-bottom: 2rem;">Downstream Tasks</h2>
      <div id="downstream-carousel" class="carousel results-carousel">
        <div class="item slide-in-left downstream-card">
          <img src="figures/Figure_1.png" alt="Video Object Segmentation Results" />
          <h2 class="subtitle has-text-centered downstream-caption">
            Video Object Segmentation on DAVIS
          </h2>
        </div>
        <div class="item slide-in-right downstream-card">
          <img src="figures/Figure_2.png" alt="Semantic Part Propagation Results" />
          <h2 class="subtitle has-text-centered downstream-caption">
            Semantic Part Propagation on VIP
          </h2>
        </div>
        <div class="item slide-in-left downstream-card">
          <img src="figures/Figure_3.png" alt="Pose Propagation Results" />
          <h2 class="subtitle has-text-centered downstream-caption">
            Pose Propagation on JHMDB
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End downstream tasks -->

  <!-- Main Results Table -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3 fade-in-up">Performance Comparison in Visual Correspondance tasks</h2>
          <div class="content scale-in">
            <p>FRAME substantially outperforms existing self-supervised methods on visual correspondence tasks which indicates feature consistency.</p>

            <table class="table is-bordered is-striped is-narrow is-hoverable">
              <thead>
                <tr>
                  <th>Method</th>
                  <th>Backbone</th>
                  <th>DAVIS (J&F)</th>
                  <th>VIP (mIoU)</th>
                  <th>JHMDB (PCK@0.1)</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>DINO</td>
                  <td>ViT-S/16</td>
                  <td>61.8</td>
                  <td>36.2</td>
                  <td>45.6</td>
                </tr>
                <tr>
                  <td>SiamMAE</td>
                  <td>ViT-S/16</td>
                  <td>62.0</td>
                  <td>37.3</td>
                  <td>47.0</td>
                </tr>
                <tr class="has-background-success-light">
                  <td><strong>FRAME (ours)</strong></td>
                  <td><strong>ViT-S/16</strong></td>
                  <td><strong>65.7</strong></td>
                  <td><strong>41.2</strong></td>
                  <td><strong>48.7</strong></td>
                </tr>
                <tr>
                  <td>DINO</td>
                  <td>ViT-S/8</td>
                  <td>69.9</td>
                  <td>39.5</td>
                  <td>56.5</td>
                </tr>
                <tr>
                  <td>SiamMAE</td>
                  <td>ViT-S/8</td>
                  <td>71.4</td>
                  <td>45.9</td>
                  <td>61.9</td>
                </tr>
                <tr class="has-background-success-light">
                  <td><strong>FRAME (ours)</strong></td>
                  <td><strong>ViT-S/8</strong></td>
                  <td><strong>73.2</strong></td>
                  <td><strong>47.9</strong></td>
                  <td><strong>64.1</strong></td>
                </tr>
              </tbody>
            </table>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Qualitative Results -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3 fade-in-up">Qualitative Examples</h2>
          <div class="scale-in">
            <img src="figures/FRAME_teaser.png" alt="FRAME vs DINO Qualitative Results" style="width: 100%;">
          </div>
          <h2 class="subtitle has-text-centered slide-in-right">
            Comparison of FRAME and DINO on feature propagation across video frames. FRAME demonstrates greater
            robustness to <strong>viewpoint changes, occlusions, and object reappearances </strong>, making it a more suitable video frame
            encoder.
          </h2>
        </div>
      </div>
    </div>
  </section>

  <!-- DINO vs FRAME Comparison -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3 has-text-centered fade-in-up">DINO (Left) vs FRAME (Right) - Comparison</h2>
        <div id="dino-frame-carousel" class="carousel results-carousel">
          <div class="item item-video1 has-text-centered scale-in">
            <video poster="" id="dino-frame1" autoplay controls muted loop class="is-centered">
              <source src="static/videos/dino_vs_frame/0_html5.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video2 has-text-centered scale-in">
            <video poster="" id="dino-frame2" autoplay controls muted loop class="is-centered">
              <source src="static/videos/dino_vs_frame/2_html5.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video3 has-text-centered scale-in">
            <video poster="" id="dino-frame3" autoplay controls muted loop class="is-centered">
              <source src="static/videos/dino_vs_frame/11_html5.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video4 has-text-centered scale-in">
            <video poster="" id="dino-frame4" autoplay controls muted loop class="is-centered">
              <source src="static/videos/dino_vs_frame/12_html5.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video5 has-text-centered scale-in">
            <video poster="" id="dino-frame5" autoplay controls muted loop class="is-centered">
              <source src="static/videos/dino_vs_frame/15_html5.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video6 has-text-centered scale-in">
            <video poster="" id="dino-frame6" autoplay controls muted loop class="is-centered">
              <source src="static/videos/dino_vs_frame/16_html5.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video7 has-text-centered scale-in">
            <video poster="" id="dino-frame7" autoplay controls muted loop class="is-centered">
              <source src="static/videos/dino_vs_frame/19_html5.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video8 has-text-centered scale-in">
            <video poster="" id="dino-frame8" autoplay controls muted loop class="is-centered">
              <source src="static/videos/dino_vs_frame/21_html5.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video9 has-text-centered scale-in">
            <video poster="" id="dino-frame9" autoplay controls muted loop class="is-centered">
              <source src="static/videos/dino_vs_frame/29_html5.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End DINO vs FRAME Comparison -->

  <!-- Ablation Studies -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3 fade-in-up">Insights</h2>
          <div class="content">
            <div class="scale-in">
              <table class="table is-bordered is-striped is-narrow is-hoverable">
                <thead>
                  <tr>
                    <th>Memory</th>
                    <th>Anticipation</th>
                    <th>Stages</th>
                    <th>Data</th>
                    <th>DAVIS (J&F)</th>
                    <th>VIP (mIoU)</th>
                    <th>JHMDB (PCK)</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>✗</td>
                    <td>✗</td>
                    <td>Stage 1</td>
                    <td>Kinetics</td>
                    <td>62.1</td>
                    <td>39.0</td>
                    <td>46.6</td>
                  </tr>
                  <tr>
                    <td>✓</td>
                    <td>✓</td>
                    <td>2-Stage</td>
                    <td>Kinetics</td>
                    <td>65.7</td>
                    <td>41.2</td>
                    <td>48.7</td>
                  </tr>
                  <tr>
                    <td>✓</td>
                    <td>✗</td>
                    <td>2-Stage</td>
                    <td>Kin.+Ego4D</td>
                    <td>65.5</td>
                    <td>41.2</td>
                    <td>48.6</td>
                  </tr>
                  <tr class="has-background-success-light">
                    <td><strong>✓</strong></td>
                    <td><strong>✓</strong></td>
                    <td><strong>2-Stage</strong></td>
                    <td><strong>Kin.+Ego4D</strong></td>
                    <td><strong>66.3</strong></td>
                    <td><strong>42.0</strong></td>
                    <td><strong>49.0</strong></td>
                  </tr>
                </tbody>
              </table>
            </div>

            <p class="slide-in-left">Both memory and anticipation components contribute significantly to performance, with the two-stage
              training providing the best results.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Semantic Segmentation Results -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3 fade-in-up">Semantic Segmentation and Anticipation</h2>
          <div class="content">
            <div class="columns">
              <div class="column is-7 slide-in-left">
                <img src="figures/step_anticipation.png" alt="Semantic segmentation on current and future frames" class="center">
                <p class="has-text-centered"><strong>Semantic segmentation on current and future frames.</strong></p>
              </div>
              <div class="column is-5 slide-in-right">
                <table class="table is-bordered is-striped is-narrow">
                  <thead>
                    <tr>
                      <th>Model</th>
                      <th>CamVid Current</th>
                      <th>CamVid Future</th>
                      <th>VSPW Current</th>
                      <th>VSPW Future</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>DINO ViT-S/16</td>
                      <td>60.1</td>
                      <td>50.2</td>
                      <td>36.4</td>
                      <td>25.6</td>
                    </tr>
                    <tr class="has-background-success-light">
                      <td><strong>FRAME ViT-S/8</strong></td>
                      <td><strong>62.6</strong></td>
                      <td><strong>54.0</strong></td>
                      <td><strong>38.0</strong></td>
                      <td><strong>27.4</strong></td>
                    </tr>
                    <tr>
                      <td>DINOv2 ViT-L/14</td>
                      <td>68.3</td>
                      <td>56.1</td>
                      <td>41.8</td>
                      <td>30.3</td>
                    </tr>
                    <tr class="has-background-success-light">
                      <td><strong>FRAME ViT-L/14</strong></td>
                      <td><strong>69.8</strong></td>
                      <td><strong>59.2</strong></td>
                      <td><strong>44.0</strong></td>
                      <td><strong>33.8</strong></td>
                    </tr>
                  </tbody>
                </table>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Comparison of Performance vs Parameters -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3 fade-in-up">Performance vs Parameters</h2>
          <div class="content">
            <div class="scale-in">
              <img src="figures/performance_chart_1.png" alt="FRAME outperforms DINO with fewer parameters." class="center">
            </div>
            <p class="has-text-centered slide-in-left"><strong>FRAME outperforms DINO with fewer parameters.</strong></p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 fade-in-up">Bibtex</h2>
          <div class="content slide-in-left">
            <p style="margin-bottom: 1.5rem;">If you find this work useful, please consider citing:</p>
            <pre style="text-align: left; background-color: #f5f5f5; padding: 1.5rem; border-radius: 8px; font-size: 0.9rem;"><code>@misc{tv2025framepretrainingvideofeature,
      title={FRAME: Pre-Training Video Feature Representations via Anticipation and Memory}, 
      author={Sethuraman TV and Savya Khosla and Vignesh Srinivasakumar and Jiahui Huang and Seoung Wug Oh and Simon Jenni and Derek Hoiem and Joon-Young Lee},
      year={2025},
      eprint={2506.05543},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.05543}, 
}</code></pre>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!--End BibTex citation -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <script type="module">
    import { AutoProcessor, AutoModel, RawImage, Tensor, PreTrainedModel, matmul } from "https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.7.2";

    const DEMO_MODEL_ID = "onnx-community/dinov3-vits16-pretrain-lvd1689m-ONNX-MHA-scores";
    const DEMO_VIDEOS = [
      {
        id: "demo-12927739",
        label: "Demo 1",
        src: "https://huggingface.co/spaces/webml-community/DINOv3-video-tracking/resolve/main/examples/12927739_60fps.mp4",
      },
      {
        id: "demo-8624901",
        label: "Demo 2",
        src: "https://huggingface.co/spaces/webml-community/DINOv3-video-tracking/resolve/main/examples/8624901_30fps.mp4",
      },
      {
        id: "demo-4411457",
        label: "Demo 3",
        src: "https://huggingface.co/spaces/webml-community/DINOv3-video-tracking/resolve/main/examples/4411457_25fps.mp4",
      },
    ];
    const DEMO_TARGET_PIXELS = 250_000;
    const DEMO_PLAYBACK_FPS = 10;
    const DEMO_SIMILARITY_THRESHOLD = 0.75;

    const demoStatus = document.getElementById("dinov3-status");
    const demoVideo = document.getElementById("dinov3-video");
    const demoOverlay = document.getElementById("dinov3-overlay");
    const demoOverlayCtx = demoOverlay?.getContext("2d");
    const demoSlider = document.getElementById("dinov3-slider");
    const demoCounter = document.getElementById("dinov3-counter");
    const demoPlayBtn = document.getElementById("dinov3-play");
    const demoPicker = document.getElementById("dinov3-picker");

    let demoProcessor;
    let demoModel;
    let demoPostModel;
    let demoPatchSize;
    let demoOffsetTensor;

    const demoOffscreenCanvas = document.createElement("canvas");
    const demoOffscreenCtx = demoOffscreenCanvas.getContext("2d", { willReadFrequently: true });

    const demoFrameCache = new Map();
    const demoSelections = new Map();

    let demoGridInfo = null;
    let demoTotalFrames = 0;
    let demoIsPlaying = false;
    let demoAnimationId = null;
    let demoLastFrameTime = 0;
    let demoModelsLoaded = false;
    let demoLoadToken = 0;
    let demoCurrentVideoId = DEMO_VIDEOS[0]?.id ?? null;

    function updateDemoStatus(message) {
      if (demoStatus) demoStatus.textContent = message;
    }

    function setDemoControlsDisabled(disabled) {
      if (!demoPlayBtn || !demoSlider) return;
      demoPlayBtn.disabled = disabled;
      demoSlider.disabled = disabled;
    }

    function updateDemoCounter() {
      if (!demoCounter || !demoSlider) return;
      const frame = demoTotalFrames ? parseInt(demoSlider.value, 10) + 1 : 0;
      demoCounter.textContent = `Frame ${frame} / ${demoTotalFrames}`;
    }

    function getDemoVideoConfig(videoId) {
      return DEMO_VIDEOS.find((video) => video.id === videoId) ?? null;
    }

    function setActiveDemoTile(videoId) {
      if (!demoPicker) return;
      const tiles = demoPicker.querySelectorAll(".dinov3-tile");
      tiles.forEach((tile) => {
        const isActive = tile.dataset.videoId === videoId;
        tile.classList.toggle("is-active", isActive);
        tile.setAttribute("aria-pressed", isActive ? "true" : "false");
      });
    }

    function setDemoPickerDisabled(disabled) {
      if (!demoPicker) return;
      const tiles = demoPicker.querySelectorAll(".dinov3-tile");
      tiles.forEach((tile) => {
        tile.disabled = disabled;
        tile.setAttribute("aria-disabled", disabled ? "true" : "false");
      });
    }

    async function ensureDemoModels() {
      if (demoModelsLoaded) return;
      await loadDemoModels();
      demoModelsLoaded = true;
    }

    async function loadDemoModels() {
      updateDemoStatus("Loading models…");
      demoProcessor = await AutoProcessor.from_pretrained(DEMO_MODEL_ID);
      demoProcessor.image_processor.do_resize = false;

      demoModel = await AutoModel.from_pretrained(DEMO_MODEL_ID, {
        device: "webgpu",
        dtype: "fp32",
        session_options: {
          preferredOutputLocation: { last_hidden_state: "gpu-buffer" },
        },
      });

      demoPostModel = await PreTrainedModel.from_pretrained(DEMO_MODEL_ID, {
        model_file_name: "postprocess",
        dtype: "fp32",
        device: "webgpu",
      });

      demoPatchSize = demoModel.config.patch_size;
      const offsetLen = 1 + (demoModel.config.num_register_tokens ?? 0);
      demoOffsetTensor = new Tensor("int64", [offsetLen], []);
    }

    function configureDemoCanvas() {
      if (!demoOverlay || !demoVideo) return;
      demoOverlay.width = demoVideo.videoWidth;
      demoOverlay.height = demoVideo.videoHeight;

      const aspect = demoVideo.videoWidth / demoVideo.videoHeight;
      const scaledHeight = Math.sqrt(DEMO_TARGET_PIXELS / aspect);
      const scaledWidth = scaledHeight * aspect;

      demoOffscreenCanvas.width = Math.max(demoPatchSize, Math.round(scaledWidth / demoPatchSize) * demoPatchSize);
      demoOffscreenCanvas.height = Math.max(demoPatchSize, Math.round(scaledHeight / demoPatchSize) * demoPatchSize);
    }

    function prepareDemoFrames() {
      if (!demoVideo || !demoSlider) return;
      demoTotalFrames = Math.max(1, Math.floor(demoVideo.duration * DEMO_PLAYBACK_FPS));
      demoSlider.max = demoTotalFrames - 1;
      demoSlider.value = "0";
      updateDemoCounter();
    }

    async function processDemoFrames(loadToken) {
      setDemoControlsDisabled(true);
      demoFrameCache.clear();
      demoSelections.clear();
      demoGridInfo = null;

      for (let frameIndex = 0; frameIndex < demoTotalFrames; frameIndex++) {
        if (loadToken !== demoLoadToken) return false;
        updateDemoStatus(`Processing frame ${frameIndex + 1} of ${demoTotalFrames}…`);
        const time = frameIndex / DEMO_PLAYBACK_FPS;

        // eslint-disable-next-line no-await-in-loop
        const frameProcessed = await new Promise((resolve, reject) => {
          const onSeeked = async () => {
            demoVideo.removeEventListener("seeked", onSeeked);
            demoVideo.removeEventListener("error", onError);
            if (loadToken !== demoLoadToken) {
              resolve(false);
              return;
            }
            await processDemoFrame(frameIndex);
            if (loadToken !== demoLoadToken) {
              resolve(false);
              return;
            }
            resolve(true);
          };
          const onError = () => {
            demoVideo.removeEventListener("seeked", onSeeked);
            demoVideo.removeEventListener("error", onError);
            reject(new Error("Unable to seek video frame."));
          };
          demoVideo.addEventListener("seeked", onSeeked, { once: true });
          demoVideo.addEventListener("error", onError, { once: true });
          demoVideo.currentTime = Math.min(time, demoVideo.duration);
        });

        if (!frameProcessed) return false;
      }

      if (loadToken !== demoLoadToken) return false;
      if (demoSlider) demoSlider.value = "0";
      updateDemoCounter();
      await drawDemoHighlights();
      return true;
    }

    async function processDemoFrame(frameIndex) {
      const frameCanvas = document.createElement("canvas");
      frameCanvas.width = demoOverlay.width;
      frameCanvas.height = demoOverlay.height;
      frameCanvas.getContext("2d").drawImage(demoVideo, 0, 0, frameCanvas.width, frameCanvas.height);

      demoOffscreenCtx.drawImage(demoVideo, 0, 0, demoOffscreenCanvas.width, demoOffscreenCanvas.height);
      const rawImage = await RawImage.fromCanvas(demoOffscreenCanvas);
      const inputs = await demoProcessor(rawImage);

      if (!demoGridInfo) {
        const [, , h, w] = inputs.pixel_values.dims;
        demoGridInfo = {
          width: w / demoPatchSize,
          height: h / demoPatchSize,
        };
      }

      const { last_hidden_state } = await demoModel(inputs);
      const { normalized_features } = await demoPostModel({ last_hidden_state, offset: demoOffsetTensor });

      demoFrameCache.set(frameIndex, { canvas: frameCanvas, features: normalized_features });
      last_hidden_state.dispose();
    }

    function getDemoFrame() {
      if (!demoSlider) return null;
      const index = parseInt(demoSlider.value, 10);
      return demoFrameCache.get(index) ?? null;
    }

    function drawDemoBase(frameData) {
      if (!demoOverlayCtx) return;
      demoOverlayCtx.clearRect(0, 0, demoOverlay.width, demoOverlay.height);
      if (frameData) {
        demoOverlayCtx.drawImage(frameData.canvas, 0, 0, demoOverlay.width, demoOverlay.height);
      } else if (demoVideo) {
        demoOverlayCtx.drawImage(demoVideo, 0, 0, demoOverlay.width, demoOverlay.height);
      }
    }

    async function drawDemoHighlights() {
      const frameData = getDemoFrame();
      drawDemoBase(frameData);
      if (!frameData || !demoGridInfo || !demoOverlayCtx) return;

      const patchW = demoOverlay.width / demoGridInfo.width;
      const patchH = demoOverlay.height / demoGridInfo.height;

      const highlighted = await getDemoSimilarPatches();
      if (highlighted) {
        demoOverlayCtx.fillStyle = "rgba(56, 189, 248, 0.6)";
        for (const idx of highlighted) {
          const x = (idx % demoGridInfo.width) * patchW;
          const y = Math.floor(idx / demoGridInfo.width) * patchH;
          demoOverlayCtx.fillRect(x, y, patchW, patchH);
        }
      }

      if (!demoSlider) return;
      const frameIndex = parseInt(demoSlider.value, 10);
      if (demoSelections.has(frameIndex)) {
        demoOverlayCtx.fillStyle = "rgba(251, 146, 60, 0.7)";
        for (const idx of demoSelections.get(frameIndex).positive) {
          const x = (idx % demoGridInfo.width) * patchW;
          const y = Math.floor(idx / demoGridInfo.width) * patchH;
          demoOverlayCtx.fillRect(x, y, patchW, patchH);
        }
      }
    }

    async function getDemoSimilarPatches() {
      if (!demoSlider) return null;
      const currentIndex = parseInt(demoSlider.value, 10);
      const currentFrame = demoFrameCache.get(currentIndex);
      if (!currentFrame) return null;

      const collected = [];
      let selectionCount = 0;

      for (const [frameIndex, selection] of demoSelections.entries()) {
        if (!selection.positive.size) continue;
        const source = demoFrameCache.get(frameIndex);
        if (!source) continue;

        const [, , hiddenSize] = source.features.dims;
        const data = source.features.data;
        for (const idx of selection.positive) {
          const start = idx * hiddenSize;
          const end = start + hiddenSize;
          collected.push(...data.slice(start, end));
          selectionCount++;
        }
      }

      if (selectionCount === 0) return null;

      const [, , hiddenSize] = currentFrame.features.dims;
      const positives = new Tensor("float32", collected, [1, selectionCount, hiddenSize]);
      const scores = await matmul(positives, currentFrame.features.permute(0, 2, 1));
      const maxScores = scores.max(1);

      const highlighted = new Set();
      maxScores.data.forEach((value, index) => {
        if (value > DEMO_SIMILARITY_THRESHOLD) highlighted.add(index);
      });

      positives.dispose();
      scores.dispose();
      maxScores.dispose();

      return highlighted;
    }

    function handleDemoSlider() {
      if (demoIsPlaying) toggleDemoPlayback();
      updateDemoCounter();
      drawDemoHighlights();
    }

    function toggleDemoPlayback() {
      if (demoFrameCache.size === 0) return;
      demoIsPlaying = !demoIsPlaying;
      if (demoPlayBtn) demoPlayBtn.textContent = demoIsPlaying ? "Pause" : "Play";

      if (demoIsPlaying) {
        demoLastFrameTime = performance.now();
        demoAnimationId = requestAnimationFrame(runDemoPlayback);
      } else if (demoAnimationId) {
        cancelAnimationFrame(demoAnimationId);
        demoAnimationId = null;
      }
    }

    function runDemoPlayback(timestamp) {
      if (!demoIsPlaying) return;
      const interval = 1000 / DEMO_PLAYBACK_FPS;

      if (timestamp - demoLastFrameTime >= interval) {
        demoLastFrameTime = timestamp;
        if (demoSlider) {
          let frame = parseInt(demoSlider.value, 10);
          frame = (frame + 1) % demoTotalFrames;
          demoSlider.value = frame.toString();
        }
        updateDemoCounter();
        drawDemoHighlights();
      }

      demoAnimationId = requestAnimationFrame(runDemoPlayback);
    }

    async function handleDemoSelection(event) {
      if (demoFrameCache.size === 0 || !demoGridInfo) return;

      const rect = demoOverlay.getBoundingClientRect();
      const xRatio = (event.clientX - rect.left) / rect.width;
      const yRatio = (event.clientY - rect.top) / rect.height;

      const gridX = Math.floor(xRatio * demoGridInfo.width);
      const gridY = Math.floor(yRatio * demoGridInfo.height);
      const patchIndex = gridY * demoGridInfo.width + gridX;

      if (!demoSlider) return;
      const frameIndex = parseInt(demoSlider.value, 10);
      if (!demoSelections.has(frameIndex)) {
        demoSelections.set(frameIndex, { positive: new Set() });
      }
      demoSelections.get(frameIndex).positive.add(patchIndex);
      await drawDemoHighlights();
    }

    async function initializeDemo(requestedVideo = getDemoVideoConfig(demoCurrentVideoId) ?? DEMO_VIDEOS[0]) {
      if (!demoVideo || !requestedVideo) return;

      const loadToken = ++demoLoadToken;
      demoCurrentVideoId = requestedVideo.id;
      setActiveDemoTile(requestedVideo.id);
      setDemoPickerDisabled(true);
      setDemoControlsDisabled(true);
      updateDemoStatus("Preparing demo…");

      if (demoIsPlaying) toggleDemoPlayback();
      if (demoPlayBtn) demoPlayBtn.textContent = "Play";
      if (demoAnimationId) {
        cancelAnimationFrame(demoAnimationId);
        demoAnimationId = null;
      }

      try {
        if (!demoModelsLoaded) {
          await ensureDemoModels();
          if (loadToken !== demoLoadToken) return;
        }

        if (loadToken !== demoLoadToken) return;
        updateDemoStatus(`Loading "${requestedVideo.label}"…`);

        demoVideo.crossOrigin = "anonymous";
        demoVideo.pause();

        const metadataLoaded = await new Promise((resolve, reject) => {
          const cleanup = () => {
            demoVideo.removeEventListener("loadedmetadata", onLoadedMetadata);
            demoVideo.removeEventListener("error", onError);
          };

          const onLoadedMetadata = () => {
            cleanup();
            if (loadToken !== demoLoadToken) {
              resolve(false);
              return;
            }
            configureDemoCanvas();
            prepareDemoFrames();
            demoVideo.pause();
            demoVideo.currentTime = 0;
            drawDemoBase(null);
            resolve(true);
          };

          const onError = () => {
            cleanup();
            reject(new Error("Unable to load video."));
          };

          demoVideo.addEventListener("loadedmetadata", onLoadedMetadata, { once: true });
          demoVideo.addEventListener("error", onError, { once: true });
          demoVideo.src = requestedVideo.src;
          demoVideo.load();
        });

        if (!metadataLoaded || loadToken !== demoLoadToken) return;

        const processed = await processDemoFrames(loadToken);
        if (loadToken !== demoLoadToken) return;
        if (!processed) return;

        setDemoControlsDisabled(false);
        updateDemoStatus('Processing complete. Click on the video to select a target patch.');
      } catch (error) {
        if (loadToken !== demoLoadToken) return;
        console.error(error);
        updateDemoStatus(`Demo error: ${error?.message ?? "Unable to start demo."}`);
      } finally {
        if (loadToken === demoLoadToken) {
          setDemoPickerDisabled(false);
          if (demoFrameCache.size > 0) {
            setDemoControlsDisabled(false);
            updateDemoCounter();
          }
        }
      }
    }

    if (demoSlider) demoSlider.addEventListener("input", handleDemoSlider);
    if (demoPlayBtn) demoPlayBtn.addEventListener("click", toggleDemoPlayback);
    if (demoOverlay) {
      demoOverlay.addEventListener("mousedown", handleDemoSelection);
      demoOverlay.addEventListener("contextmenu", (event) => event.preventDefault());
    }
    if (demoPicker) {
      demoPicker.querySelectorAll(".dinov3-tile").forEach((tile) => {
        tile.addEventListener("click", () => {
          if (tile.disabled) return;
          const videoId = tile.dataset.videoId;
          const config = getDemoVideoConfig(videoId);
          if (!config) return;
          initializeDemo(config).catch((error) => {
            console.error(error);
            updateDemoStatus(`Demo error: ${error?.message ?? "Unable to start demo."}`);
          });
        });
      });
    }

    initializeDemo();
  </script>

</body>

</html>
